{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data size < 1MB, hence loading the entire content into memeory is feasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the block below, I collect and count the number of occurances of each word in the corpurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>In the below block, in the line - \"for line in open(....)\", please replace 'training.data' with the relative path of the training file</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allWords=dict() # dict stores the count of occurances of every word in the corpus\n",
    "sentences=[]    # Tokenized sentences \n",
    "label1=[]       # Collecting labels\n",
    "label2=[]       # Collecting labels\n",
    "lab1Count=1     \n",
    "lab2Count=1\n",
    "label1dict=dict()  # Assigning numerical values to labels\n",
    "label2dict=dict()  # Assigning numerical values to labels\n",
    "\n",
    "for line in open('training.data','r',encoding=\"ISO-8859-1\"):\n",
    "    line=line.strip().split(' ')\n",
    "    labels=line[0].split(':')\n",
    "    if(len(labels)!=2 or ' ' in line[0]):\n",
    "        print('Formatting asssumption wrong')\n",
    "        break\n",
    "    sentence=[word for word in line[1:] if word not in punctuation]\n",
    "    sentences.append(list(sentence))\n",
    "    for word in list(sentence):\n",
    "        if(allWords.get(word,0)==0):\n",
    "            allWords[word]=1\n",
    "        else:\n",
    "            allWords[word]+=1\n",
    "    if(not(label1dict.get(labels[0],0))):\n",
    "        label1dict[labels[0]]=lab1Count\n",
    "        lab1Count+=1\n",
    "    if(not(label2dict.get(labels[1],0))):\n",
    "        label2dict[labels[1]]=lab2Count\n",
    "        lab2Count+=1\n",
    "    label1.append(label1dict[labels[0]])\n",
    "    label2.append(label2dict[labels[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the block below, I collect the 500 most frequently occuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "popularTokens=[x[0] for x in sorted(allWords.items(),key=lambda x:x[1],reverse=True)[:500]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function converts each sentence into a 500 long vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordVec(sent):\n",
    "    vec=[]\n",
    "    for word in popularTokens:\n",
    "        if word in sent:\n",
    "            vec.append(1)\n",
    "        else:\n",
    "            vec.append(0)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectors is a list that contains the 500-long vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors=[wordVec(eachSentence) for eachSentence in sentences]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training set\n",
    "trainVectors=np.array(vectors[:int(0.6*len(vectors))])\n",
    "trainLabels1=np.array(label1[:int(0.6*len(vectors))])\n",
    "trainLabels2=np.array(label2[:int(0.6*len(vectors))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Validation Set\n",
    "validVectors=np.array(vectors[int(0.6*len(vectors)):int(0.8*len(vectors))])\n",
    "validLabels1=np.array(label1[int(0.6*len(vectors)):int(0.8*len(vectors))])\n",
    "validLabels2=np.array(label2[int(0.6*len(vectors)):int(0.8*len(vectors))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test Set\n",
    "testVectors=np.array(vectors[int(0.8*len(vectors)):])\n",
    "testLabels1=np.array(label1[int(0.8*len(vectors)):])\n",
    "testLabels2=np.array(label2[int(0.8*len(vectors)):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am training two classifiers, one for the primary label that occurs before the ':' and the other is for the \n",
    "secondary label.\n",
    "Using soft margin SVM with a varying cost parameters before fixing on one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8809061488673139\n",
      "0.9080906148867314\n",
      "0.9122977346278317\n"
     ]
    }
   ],
   "source": [
    "for c in [100,1000,10**5]:\n",
    "    clf=svm.SVC(C=c, kernel='rbf', gamma='auto', tol=0.001, class_weight='balanced', max_iter=-1,\\\n",
    "                decision_function_shape='ovr')\n",
    "    clf.fit(trainVectors,trainLabels1)\n",
    "    preds=clf.predict(validVectors)\n",
    "    accu=sum([1 if i==j else 0 for i,j in zip(preds,validLabels1)])/len(validLabels1)\n",
    "    print(accu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the primary label, I decide to use an SVM with C=10**5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporting the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8670333225493367\n"
     ]
    }
   ],
   "source": [
    "clf=svm.SVC(C=10**5, kernel='rbf', gamma='auto', tol=0.001, class_weight='balanced', max_iter=-1,\\\n",
    "                decision_function_shape='ovr')\n",
    "clf.fit(np.vstack([trainVectors,validVectors]),np.hstack([trainLabels1,validLabels1]))\n",
    "preds=clf.predict(testVectors)\n",
    "accu=sum([1 if i==j else 0 for i,j in zip(preds,testLabels1)])/len(testLabels1)\n",
    "print('Test accuracy obtained for primary label is ',accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if it is actually working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label1dictReverse=dict((i,j) for i,j in zip(list(label1dict.values()),list(label1dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'developed', 'a', 'crack', 'in', '1835', 'while', 'tolling', 'the', 'death', 'of', 'U.S.', 'Chief', 'Justice', 'John', 'Marshall']\n",
      "predicted label is  ENTY\n",
      "actual label is  ENTY\n",
      "['What', 'was', 'the', 'name', 'of', 'the', 'American', 'ship', 'sunk', 'by', 'a', 'mine', 'in', 'Havana', 'harbor', 'causing', 'the', 'Spanish-American', 'war']\n",
      "predicted label is  ENTY\n",
      "actual label is  ENTY\n",
      "['Who', 'is', 'Prince', 'Naseem', 'Hamed']\n",
      "predicted label is  HUM\n",
      "actual label is  HUM\n",
      "['What', 'is', 'the', 'name', 'of', 'the', 'vaccine', 'for', 'chicken', 'pox']\n",
      "predicted label is  ENTY\n",
      "actual label is  ENTY\n",
      "['How', 'do', 'I', 'start', 'a', 'bank']\n",
      "predicted label is  DESC\n",
      "actual label is  DESC\n",
      "['What', 'are', 'the', '7', 'articles', 'of', 'the', 'constitution']\n",
      "predicted label is  ENTY\n",
      "actual label is  ENTY\n",
      "['Which', 'of', 'the', 'following', 'was', 'Rhodes', 'Scholar']\n",
      "predicted label is  HUM\n",
      "actual label is  HUM\n",
      "['What', 'is', 'the', 'temperature', 'for', 'baking', 'Peachy', 'Oat', 'Muffins']\n",
      "predicted label is  NUM\n",
      "actual label is  NUM\n",
      "['What', \"'s\", 'the', 'colored', 'part', 'of', 'the', 'eye', 'called']\n",
      "predicted label is  ENTY\n",
      "actual label is  ENTY\n",
      "['What', 'is', 'film', 'noir']\n",
      "predicted label is  DESC\n",
      "actual label is  DESC\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    randNum=random.randint(0,len(vectors))\n",
    "    print(sentences[randNum])\n",
    "    print('predicted label is ',label1dictReverse[clf.predict([vectors[randNum]])[0]])\n",
    "    print('actual label is ',label1dictReverse[label1[randNum]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it actually is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, 1 Gram results in a test accuracy of 86.7%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the process for the secondary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7964401294498382\n",
      "0.8766990291262136\n",
      "0.8919093851132686\n"
     ]
    }
   ],
   "source": [
    "for c in [100,1000,10**5]:\n",
    "    clf=svm.SVC(C=c, kernel='rbf', gamma='auto', tol=0.001, class_weight='balanced', max_iter=-1,\\\n",
    "                decision_function_shape='ovr')\n",
    "    clf.fit(trainVectors,trainLabels2)\n",
    "    preds=clf.predict(validVectors)\n",
    "    accu=sum([1 if i==j else 0 for i,j in zip(preds,validLabels2)])/len(validLabels2)\n",
    "    print(accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decide to use c=10**5 to train svm for the secondary label as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8052410223228729\n"
     ]
    }
   ],
   "source": [
    "clf=svm.SVC(C=10**5, kernel='rbf', gamma='auto', tol=0.001, class_weight='balanced', max_iter=-1,\\\n",
    "                decision_function_shape='ovr')\n",
    "clf.fit(np.vstack([trainVectors,validVectors]),np.hstack([trainLabels2,validLabels2]))\n",
    "preds=clf.predict(testVectors)\n",
    "accu=sum([1 if i==j else 0 for i,j in zip(preds,testLabels2)])/len(testLabels2)\n",
    "print(\"Test accuracy obtained for secondary label is\",accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy greatly surprises me. I believe fastText, which I plan to use next, is the best in class and results in ~89% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with ensembles results in similar numbers but with much greater training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8657392429634423\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=500, min_samples_split=2, \\\n",
    "                            min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \\\n",
    "                            n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight='balanced')\n",
    "\n",
    "\n",
    "clf.fit(trainVectors,trainLabels1)\n",
    "\n",
    "preds=clf.predict(testVectors)\n",
    "accu=sum([1 if i==j else 0 for i,j in zip(preds,testLabels1)])/len(testLabels1)\n",
    "print(accu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
