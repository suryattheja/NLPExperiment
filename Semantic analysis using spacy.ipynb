{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import spacy\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> The following code requires spacy to be installed. It also requires the download of its english datasets, available on the spacy website. Please replace the path string below with the absolute path of the downloaded english dataset of spacy</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"/usr/local/lib/python2.7/dist-packages/spacy/data/en/en_core_web_sm-2.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please replace 'training.data' in the open() line with the relative path of the training file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allWords=dict()   \n",
    "sentences=[]\n",
    "label1=[]\n",
    "label2=[]\n",
    "lab1Count=1\n",
    "lab2Count=1\n",
    "label1dict=dict()\n",
    "label2dict=dict()\n",
    "\n",
    "for line in open('training.data','r',encoding=\"ISO-8859-1\"):\n",
    "    line=[word.lower() for word in line.strip().split(' ')]\n",
    "    labels=line[0].split(':')\n",
    "    if(len(labels)!=2 or ' ' in line[0]):\n",
    "        print('Formatting asssumption wrong')\n",
    "        break\n",
    "    sentence=[word for word in line[1:] if word not in punctuation]\n",
    "    sentences.append(sentence)\n",
    "    if(not(label1dict.get(labels[0],0))):\n",
    "        label1dict[labels[0]]=lab1Count\n",
    "        lab1Count+=1\n",
    "    if(not(label2dict.get(labels[1],0))):\n",
    "        label2dict[labels[1]]=lab2Count\n",
    "        lab2Count+=1\n",
    "    label1.append(label1dict[labels[0]])\n",
    "    label2.append(label2dict[labels[1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic principle is: The algorithm only depends on the properties of the question word and its neighbour. \n",
    "In each sentence, I look for the question word using spacy. Then I represent each sentence using a vector that has the structure: [POS of the Q-word, the Q-word, POS of the next word, the next word]. I have never tried this approach before and have borrowed the idea from  https://nlp.stanford.edu/courses/cs224n/2010/reports/olalerew.pdf. However I have ignored several aspects of the methodology. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentencesStr=[\" \".join(sentences[i]) for i in range(len(sentences))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qTag=dict()\n",
    "qTagInd=1\n",
    "qText=dict()\n",
    "qTextInd=1\n",
    "qBigram=dict()\n",
    "qBigramInd=1\n",
    "qNbrPos=dict()\n",
    "qNbrInd=1\n",
    "rootTag=dict()\n",
    "rootTagInd=1\n",
    "allVectors=[]\n",
    "count=1\n",
    "allBigrams=[]\n",
    "\n",
    "for sentence in sentencesStr:\n",
    "    sentenceVector=[None]*4\n",
    "    tokens=nlp(sentence)\n",
    "    for token in tokens:\n",
    "        \n",
    "        if token.tag_ == \"WDT\" or token.tag_ == \"WP\" or token.tag_ == \"WP$\" or token.tag_ == \"WRB\":\n",
    "            if(token.tag_ not in qTag):\n",
    "                qTag[token.tag_]=qTagInd\n",
    "                qTagInd+=1\n",
    "            sentenceVector[0]=qTag[token.tag_]\n",
    "            \n",
    "            if(token.text not in qText):\n",
    "                qText[token.text]=qTextInd\n",
    "                qTextInd+=1\n",
    "            sentenceVector[1]=qText[token.text]\n",
    "                \n",
    "            if(token.i<len(tokens)-1):\n",
    "                biGram=(str(tokens[token.i]),str(tokens[token.i+1]))\n",
    "                if biGram not in qBigram:\n",
    "                    allBigrams.append(biGram)\n",
    "                    qBigram[biGram]=qBigramInd\n",
    "                    qBigramInd+=1\n",
    "                sentenceVector[2]=qBigram[biGram]\n",
    "                    \n",
    "                if(tokens[token.i+1].tag_ not in qNbrPos):\n",
    "                    qNbrPos[tokens[token.i+1].tag_]=qNbrInd\n",
    "                    qNbrInd+=1\n",
    "                sentenceVector[2]=qNbrPos[tokens[token.i+1].tag_]\n",
    "                    \n",
    "        if token.dep_ == \"ROOT\":\n",
    "            if token.tag_ not in rootTag:\n",
    "                rootTag[token.tag_]=rootTagInd\n",
    "                rootTagInd+=1\n",
    "            sentenceVector[3]=rootTag[token.tag_]\n",
    "    sentenceVector=[0 if word==None else word for word in sentenceVector]\n",
    "    allVectors.append(sentenceVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function converts each vector described above into a 66 dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeVectors(vec):\n",
    "    rep=[0]*(62+4)\n",
    "    rep[vec[0]]=1\n",
    "    rep[5+vec[1]]=1\n",
    "    rep[20+vec[2]]=1\n",
    "    rep[44+vec[3]]=1\n",
    "    return rep\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors=[]\n",
    "for vec in allVectors:\n",
    "    vectors.append(makeVectors(vec))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set\n",
    "trainVectors=np.array(vectors[:int(0.6*len(vectors))])\n",
    "trainLabels1=np.array(label1[:int(0.6*len(vectors))])\n",
    "trainLabels2=np.array(label2[:int(0.6*len(vectors))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Validation Set\n",
    "validVectors=np.array(vectors[int(0.6*len(vectors)):int(0.8*len(vectors))])\n",
    "validLabels1=np.array(label1[int(0.6*len(vectors)):int(0.8*len(vectors))])\n",
    "validLabels2=np.array(label2[int(0.6*len(vectors)):int(0.8*len(vectors))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test Set\n",
    "testVectors=np.array(vectors[int(0.8*len(vectors)):])\n",
    "testLabels1=np.array(label1[int(0.8*len(vectors)):])\n",
    "testLabels2=np.array(label2[int(0.8*len(vectors)):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.610032362459547\n",
      "0.6203883495145631\n",
      "0.6229773462783171\n"
     ]
    }
   ],
   "source": [
    "for c in [100,1000,10**5]:\n",
    "    clf=svm.SVC(C=c, kernel='rbf', gamma='auto', tol=0.001, class_weight='balanced', max_iter=-1,\\\n",
    "                decision_function_shape='ovr')\n",
    "    clf.fit(trainVectors,trainLabels1)\n",
    "    preds=clf.predict(validVectors)\n",
    "    accu=sum([1 if i==j else 0 for i,j in zip(preds,validLabels1)])/len(validLabels1)\n",
    "    print(accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy obtained for primary label is  0.620834681332902\n"
     ]
    }
   ],
   "source": [
    "clf=svm.SVC(C=10**5, kernel='rbf', gamma='auto', tol=0.001, class_weight='balanced', max_iter=-1,\\\n",
    "                decision_function_shape='ovr')\n",
    "clf.fit(np.vstack([trainVectors,validVectors]),np.hstack([trainLabels1,validLabels1]))\n",
    "preds=clf.predict(testVectors)\n",
    "accu=sum([1 if i==j else 0 for i,j in zip(preds,testLabels1)])/len(testLabels1)\n",
    "print('Test accuracy obtained for primary label is ',accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33980582524271846\n",
      "0.3423948220064725\n",
      "0.34368932038834954\n"
     ]
    }
   ],
   "source": [
    "for c in [100,1000,10**5]:\n",
    "    clf=svm.SVC(C=c, kernel='rbf', gamma='auto', tol=0.001, class_weight='balanced', max_iter=-1,\\\n",
    "                decision_function_shape='ovr')\n",
    "    clf.fit(trainVectors,trainLabels2)\n",
    "    preds=clf.predict(validVectors)\n",
    "    accu=sum([1 if i==j else 0 for i,j in zip(preds,validLabels2)])/len(validLabels2)\n",
    "    print(accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy obtained for secondary label is 0.33904885150436753\n"
     ]
    }
   ],
   "source": [
    "clf=svm.SVC(C=10**5, kernel='rbf', gamma='auto', tol=0.001, class_weight='balanced', max_iter=-1,\\\n",
    "                decision_function_shape='ovr')\n",
    "clf.fit(np.vstack([trainVectors,validVectors]),np.hstack([trainLabels2,validLabels2]))\n",
    "preds=clf.predict(testVectors)\n",
    "accu=sum([1 if i==j else 0 for i,j in zip(preds,testLabels2)])/len(testLabels2)\n",
    "print(\"Test accuracy obtained for secondary label is\",accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the representation that I tried was inadequate. I am unable to improve the performance of the model by including a few other meta parameters of the sentence. I have not included the code of the same, in the interest of code cleanliness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
